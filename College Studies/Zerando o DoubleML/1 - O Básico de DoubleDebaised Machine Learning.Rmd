---
title: "1 - O Básico de DoubleDebaised Machine Learning"
author: "Hicham Munir Tayfour"
output:
  html_document:
    theme: flatly       
    toc: true           
    toc_float: true     
    code_folding: hide  
    highlight: tango    
---

A seguir, fornecemos um breve resumo e motivação para o framework de double machine learning (DML) e mostramos como os métodos correspondentes fornecidos pelo pacote [**DoubleML**](https://docs.doubleml.org/stable/index.html#doubleml-package) podem ser aplicados. Para detalhes, referimos a Chernozhukov et al. (2018).

::: callout-note
Notebooks detalhados contendo o código completo para os exemplos podem ser encontrados na [Galeria de Exemplos](https://docs.doubleml.org/stable/examples/index.html).
:::

## 1.1. Processo Gerador de Dados

Consideramos o seguinte modelo parcialmente linear

$$
y_i = \theta_0 d_i + g_0(x_i) + \zeta_i, \quad \zeta_i \sim \mathcal{N}(0,1),
$$

$$
d_i = m_0(x_i) + v_i, \quad v_i \sim \mathcal{N}(0,1),
$$

com covariáveis $x_i \sim \mathcal{N}(0, \Sigma)$, onde $\Sigma$ é uma matriz com entradas $\Sigma_{kj} = 0.7^{|j-k|}$. Estamos interessados em realizar inferência válida sobre o parâmetro causal $\theta_0$. O parâmetro verdadeiro $\theta_0$ é definido como $0.5$ em nosso experimento de simulação.

As funções de nuisance são dadas por

$$
m_0(x_i) = x_{i,1} + \frac{1}{4}  \frac{\exp(x_{i,3})}{1+\exp(x_{i,3})},
$$

$$
g_0(x_i) = \frac{\exp(x_{i,1})}{1+\exp(x_{i,1})} + \frac{1}{4} x_{i,3}.
$$

::: callout-note
-   Em Python, os dados podem ser gerados com `doubleml.datasets.make_plr_CCDDHNR2018`.
-   Em R, os dados podem ser gerados com [`DoubleML::make_plr_CCDDHNR2018()`](https://docs.doubleml.org/r/stable/reference/make_plr_CCDDHNR2018.html).
:::

```{r message = FALSE, warning = FALSE}
library(DoubleML)
set.seed(1234)
n_rep = 1000
n_obs = 500
n_vars = 20
alpha = 0.5

data = list()
for (i_rep in seq_len(n_rep)) {
    data[[i_rep]] = make_plr_CCDDHNR2018(alpha=alpha, n_obs=n_obs, dim_x=n_vars,
                                        return_type="data.frame")
}
```

## 1.2. Viés de Regularização em Abordagens ML Simples

A inferência ingênua baseada em uma aplicação direta de métodos de aprendizado de máquina para estimar o parâmetro causal, $\theta_0$, é geralmente inválida. O uso de métodos de aprendizado de máquina introduz um viés que surge devido à regularização. Uma abordagem ML simples é dada pela divisão aleatória da amostra em duas partes. Na amostra auxiliar indexada por $i \in I^C$, a função de nuisance $g_0(X)$ é estimada com um método ML, por exemplo, um learner de random forest. Dada a estimativa $\hat{g}_0(X)$, a estimativa final de $\theta_0$ é obtida como ($n=N/2$) usando a outra metade das observações indexadas com $i \in I$

$$
\hat{\theta}_0 = \left(\frac{1}{n} \sum_{i\in I} D_i^2\right)^{-1} \frac{1}{n} \sum_{i\in I} D_i (Y_i - \hat{g}_0(X_i)).
$$

A figura a seguir mostra a distribuição das estimativas resultantes $\hat{\theta}_0$ para a abordagem ML simples (os notebooks correspondentes estão disponíveis na [Galeria de Exemplos](https://docs.doubleml.org/stable/examples/index.html)).

![Distribuição com score não-ortogonal](https://docs.doubleml.org/stable/_images/r_non_orthogonal.svg){fig-align="center"}

O viés de regularização na abordagem ML simples é causado pela convergência lenta de $\hat{\theta}_0$

$$
|\sqrt{n} (\hat{\theta}_0 - \theta_0) | \rightarrow_{P} \infty
$$

isto é, mais lenta que $1/\sqrt{n}$. O fator determinante é o viés que surge ao aprender $g$ com uma random forest ou qualquer outra técnica ML. Uma ilustração heurística é dada por

$$
\sqrt{n}(\hat{\theta}_0 - \theta_0) = \underbrace{\left(\frac{1}{n} \sum_{i\in I} D_i^2\right)^{-1} \frac{1}{n} \sum_{i\in I} D_i \zeta_i}_{=:a} + \underbrace{\left(\frac{1}{n} \sum_{i\in I} D_i^2\right)^{-1} \frac{1}{n} \sum_{i\in I} D_i (g_0(X_i) - \hat{g}_0(X_i))}_{=:b}.
$$

$a$ é aproximadamente Gaussiano sob condições moderadas. No entanto, $b$ (o viés de regularização) diverge em geral.

## 1.3. Superando o Viés de Regularização por Ortogonalização

Para superar o viés de regularização, podemos eliminar parcialmente o efeito de $X$ de $D$ para obter o regressor ortogonalizado $V = D - m(X)$. Então usamos a estimativa final

$$
\check{\theta}_0 = \left(\frac{1}{n} \sum_{i\in I} \hat{V}_i D_i\right)^{-1} \frac{1}{n} \sum_{i\in I} \hat{V}_i (Y_i - \hat{g}_0(X_i)).
$$

A figura a seguir mostra a distribuição das estimativas resultantes $\hat{\theta}_0$ sem divisão de amostra (os notebooks correspondentes estão disponíveis na [Galeria de Exemplos](https://docs.doubleml.org/stable/examples/index.html)).

![Distribuição sem divisão de amostra](https://docs.doubleml.org/stable/_images/r_dml_nosplit.svg){fig-align="center"}

Se os modelos de nuisance $\hat{g}_0()$ e $\hat{m}()$ são estimados em todo o conjunto de dados, que também é usado para obter a estimativa final $\check{\theta}_0$, outro viés é observado.

## 1.4. Divisão de Amostra para Remover Viés Induzido por Overfitting

Usar divisão de amostra, isto é, estimar os modelos de nuisance $\hat{g}_0()$ e $\hat{m}()$ em uma parte dos dados (dados de treinamento) e estimar $\check{\theta}_0$ na outra parte dos dados (dados de teste), supera o viés induzido por overfitting. Podemos explorar os benefícios do cross-fitting alternando o papel das amostras de treinamento e teste. O cross-fitting tem bom desempenho empírico porque toda a amostra pode ser usada para estimação.

A figura a seguir mostra a distribuição das estimativas resultantes $\hat{\theta}_0$ com score ortogonal e divisão de amostra (os notebooks correspondentes estão disponíveis na [Galeria de Exemplos](https://docs.doubleml.org/stable/examples/index.html)).

![Distribuição com scores ortogonais e divisão de amostra](https://docs.doubleml.org/stable/_images/r_dml.svg){fig-align="center"}

## 1.5. Double/Debiased Machine Learning

Para ilustrar os benefícios da etapa de predição auxiliar no framework DML, escrevemos o erro como

$$
\sqrt{n}(\check{\theta}_0 - \theta_0) = a^* + b^* + c^*
$$

Chernozhukov et al. (2018) argumenta que:

O primeiro termo

$$
a^* := (EV^2)^{-1} \frac{1}{\sqrt{n}} \sum_{i\in I} V_i \zeta_i
$$

será assintoticamente normalmente distribuído.

O segundo termo

$$
b^* := (EV^2)^{-1} \frac{1}{\sqrt{n}} \sum_{i\in I} (\hat{m}(X_i) - m(X_i)) (\hat{g}_0(X_i) - g_0(X_i))
$$

desaparece assintoticamente para muitos processos geradores de dados.

O terceiro termo $c^*$ desaparece em probabilidade se a divisão de amostra for aplicada. Finalmente, vamos comparar todas as distribuições.

![Todas as distribuições](https://docs.doubleml.org/stable/_images/r_all.svg){fig-align="center"}

A implementação do DoubleML implementa vários scores ortogonais e aplica diretamente o cross-fitting. O código completo está disponível na [Galeria de Exemplos](https://docs.doubleml.org/stable/examples/index.html).

```{r message = FALSE, warning = FALSE}
theta_dml = rep(NA, n_rep)
se_dml = rep(NA, n_rep)

for (i_rep in seq_len(n_rep)) {
    df = data[[i_rep]]
    obj_dml_data = double_ml_data_from_data_frame(df, y_col = "y", d_cols = "d")
    obj_dml_plr = DoubleMLPLR$new(
        obj_dml_data,
        ml_l,
        ml_m,
        ml_g,
        n_folds=2,
        score='IV-type')
    obj_dml_plr$fit()
    theta_dml[i_rep] = obj_dml_plr$coef
    se_dml[i_rep] = obj_dml_plr$se
}
```

## 1.6. Score de Partialling Out

Outro estimador debiased, baseado na abordagem de partialling-out de Robinson (1988), é

$$
\check{\theta}_0 = \left(\frac{1}{n} \sum_{i\in I} \hat{V}_i \hat{V}_i \right)^{-1} \frac{1}{n} \sum_{i\in I} \hat{V}_i (Y_i - \hat{\ell}_0(X_i)),
$$

com $\ell_0(X_i) = E(Y|X)$. Todos os parâmetros de nuisance para o estimador com `score='partialling out'` são funções de média condicional, que podem ser diretamente estimadas usando métodos ML. Esta é uma pequena vantagem sobre o estimador com `score='IV-type'`. A seguir, repetimos a análise acima com `score='partialling out'`. Em uma primeira parte da análise, estimamos $\theta_0$ sem divisão de amostra. Novamente observamos um viés de overfitting.

A figura a seguir mostra a distribuição das estimativas resultantes $\hat{\theta}_0$ sem divisão de amostra (os notebooks correspondentes estão disponíveis na [Galeria de Exemplos](https://docs.doubleml.org/stable/examples/index.html)).

![Distribuição sem divisão de amostra](https://docs.doubleml.org/stable/_images/r_dml_po_nosplit.svg){fig-align="center"}

Usar divisão de amostra supera o viés induzido por overfitting.

![Distribuição com score ortogonal e divisão de amostra](https://docs.doubleml.org/stable/_images/r_dml_po.svg){fig-align="center"}

Novamente, a implementação aplica automaticamente o cross-fitting. O código completo está disponível na [Galeria de Exemplos](https://docs.doubleml.org/stable/examples/index.html).

```{r message = FALSE, warning = FALSE}
theta_dml_po = rep(NA, n_rep)
se_dml_po = rep(NA, n_rep)

for (i_rep in seq_len(n_rep)) {
    df = data[[i_rep]]
    obj_dml_data = double_ml_data_from_data_frame(df, y_col = "y", d_cols = "d")
    obj_dml_plr = DoubleMLPLR$new(
        obj_dml_data,
        ml_l,
        ml_m,
        n_folds=2,
        score='partialling out')
    obj_dml_plr$fit()
    theta_dml_po[i_rep] = obj_dml_plr$coef
    se_dml_po[i_rep] = obj_dml_plr$se
}
```

Finalmente, vamos comparar todas as distribuições.

![Todas as distribuições com score de partialling-out](https://docs.doubleml.org/stable/_images/r_po_all.svg){fig-align="center"}

## 1.7. Referências

Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018), Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68. [doi:[10.1111/ectj.12097](doi:%5B10.1111/ectj.12097){.uri}](https://doi.org/10.1111/ectj.12097).

Robinson, P. M. (1988). Root-N-consistent semi-parametric regression. Econometrica 56, 931-54. [doi:[10.2307/1912705](doi:%5B10.2307/1912705){.uri}](https://doi.org/10.2307/1912705).
