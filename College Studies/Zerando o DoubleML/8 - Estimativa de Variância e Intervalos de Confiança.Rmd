---
title: "8 - Estimativa de Variância e Intervalos de Confiança"
author: "Hicham Munir Tayfour"
output:
  html_document:
    theme: flatly       
    toc: true           
    toc_float: true     
    code_folding: hide  
    highlight: tango  
---

# 8. Estimativa de Variância e Intervalos de Confiança

## 8.1 - Estimativa de Variância

Sob condições de regularidade, o estimador $\tilde{\theta}_0$ concentra-se em uma vizinhança $1/\sqrt{N}$ de $\theta_0$ e o erro amostral $\sqrt{N}(\tilde{\theta}_0 - \theta_0)$ é aproximadamente normal com média zero e variância dada por

$$\sigma^2 := J_0^{-2} \cdot \mathbb{E}(\psi^2(W; \theta_0, \eta_0)),$$

onde $J_0 = \mathbb{E}(\psi_a(W; \eta_0))$, se a função score for linear no parâmetro $\theta$. Se o score não for linear no parâmetro $\theta$, então $J_0 = \partial_\theta\mathbb{E}(\psi(W; \theta, \eta_0)) \big|_{\theta=\theta_0}$.

Estimativas da variância são obtidas por

$$\hat{\sigma}^2 := \hat{J}_0^{-2} \cdot \mathbb{E}_N(\psi^2(W; \tilde{\theta}_0, \hat{\eta}_0)),$$

para funções score sendo lineares no parâmetro $\theta$. Para funções score não lineares, a implementação assume que derivadas e esperanças são intercambiáveis, de modo que

$$\hat{J}_0 := \mathbb{E}_N(\partial_\theta \psi(W; \tilde{\theta}_0, \hat{\eta}_0)).$$

Um intervalo de confiança aproximado é dado por

$$\left[ \tilde{\theta}_0 \pm \Phi^{-1}(1 - \alpha/2) \cdot \hat{\sigma}/\sqrt{N} \right].$$

Como exemplo, consideramos um modelo de regressão parcialmente linear (PLR) implementado em `DoubleMLPLR`.

```{r, message = FALSE, warning = FALSE}

library(DoubleML)

library(mlr3)

library(mlr3learners)

library(data.table)

lgr::get_logger("mlr3")$set_threshold("warn")

learner = lrn("regr.ranger", num.trees = 100, mtry = 20, min.node.size = 2, max.depth = 5)

ml_l = learner$clone()

ml_m = learner$clone()

set.seed(3141)

obj_dml_data = make_plr_CCDDHNR2018(alpha = 0.5)

dml_plr_obj = DoubleMLPLR$new(obj_dml_data, ml_l, ml_m)

dml_plr_obj$fit()

```

O método `fit()` de `DoubleMLPLR` armazena a estimativa $\tilde{\theta}_0$ em seu atributo `coef`.

```{r, message = FALSE, warning = FALSE}

print(dml_plr_obj$coef)

```

O erro padrão assintótico $\hat{\sigma}/\sqrt{N}$ é armazenado em seu atributo `se`.

```{r, message = FALSE, warning = FALSE}

print(dml_plr_obj$se)

```

Adicionalmente, o valor da estatística $t$ e o valor-p correspondente são fornecidos nos atributos `t_stat` e `pval`.

```{r, message = FALSE, warning = FALSE}

print(dml_plr_obj$t_stat)

print(dml_plr_obj$pval)

```

**Nota:** No R, um resumo pode ser obtido usando o método `summary()`. O método `confint()` realiza a estimação de intervalos de confiança.

```{r, message = FALSE, warning = FALSE}

dml_plr_obj$summary()

dml_plr_obj$confint()

```

Uma visão geral mais detalhada do modelo ajustado, suas especificações e o resumo pode ser obtida via a representação em string do objeto.

```{r, message = FALSE, warning = FALSE}

print(dml_plr_obj)

```

## 8.2 - Faixas de Confiança e Bootstrap com Multiplicadores para Inferência Simultânea Válida

[DoubleML](../index.html#doubleml-package) fornece métodos para realizar inferência simultânea válida para múltiplas variáveis de tratamento.

Como exemplo, considere um PLR com $p_1$ parâmetros causais de interesse $\theta_{0,1}, \ldots, \theta_{0,p_1}$ associados com variáveis de tratamento $D_1, \ldots, D_{p_1}$. A inferência sobre múltiplos coeficientes alvo pode ser realizada aplicando iterativamente o procedimento de inferência DML sobre as variáveis alvo de interesse: Cada um dos coeficientes de interesse, $\theta_{0,j}$, com $j \in \lbrace 1, \ldots, p_1 \rbrace$, resolve uma condição de momento correspondente

$$\mathbb{E}[\psi_j(W; \theta_{0,j}, \eta_{0,j})] = 0.$$

Analogamente ao caso com um único parâmetro de interesse, o modelo PLR com múltiplas variáveis de tratamento inclui duas etapas de regressão para alcançar ortogonalidade. Primeiro, a regressão principal é dada por

$$\ell_{0,j}(X) := \mathbb{E}[Y - \theta_{0,j} D_j | [D_k, X] ]$$

com $[D_k, X]$ sendo uma matriz compreendendo os confundidores, $X$, e todas as variáveis de tratamento restantes $D_k$ com $k \in \lbrace 1, \ldots, p_1\rbrace \setminus j$, por padrão. Segundo, a relação entre a variável de tratamento $D_j$ e as variáveis explanatórias restantes é determinada pela equação

$$m_{0,j}(X) := \mathbb{E}[D_j | [D_k, X] ].$$

Para mais detalhes, referimos a Belloni et al. (2018). A inferência simultânea pode ser baseada em um procedimento de bootstrap com multiplicadores introduzido em Chernozhukov et al. (2013, 2014). Alternativamente, abordagens de correção tradicionais, por exemplo a correção de Bonferroni, podem ser usadas para ajustar valores-p.

O método `bootstrap()` fornece uma implementação de um bootstrap com multiplicadores para modelos de double machine learning. Para $b=1, \ldots, B$ pesos $\xi_{i, b}$ são gerados de acordo com um bootstrap normal (Gaussiano), bootstrap wild ou bootstrap exponencial. O número de amostras bootstrap é fornecido como entrada `n_rep_boot` e para `method` pode-se escolher `'Bayes'`, `'normal'` ou `'wild'`.

Baseado nas estimativas dos erros padrão $\hat{\sigma}_j$ e $\hat{J}_{0,j} = \mathbb{E}_N(\psi_{a,j}(W; \eta_{0,j}))$ que são obtidas do DML, construímos estatísticas t bootstrap $t^{*,b}_j$ para $j=1, \ldots, p_1$

$$t^{*,b}_j := \frac{1}{\hat{\sigma}_j} \mathbb{E}_N \left[ \xi_{i,b} \cdot \psi_j(W_i; \tilde{\theta}_{0,j}, \hat{\eta}_{0,j}) \right].$$

A saída do bootstrap com multiplicadores pode ser usada para determinar a constante, $c_{1-\alpha}$ que é necessária para a construção de uma banda de confiança simultânea $(1-\alpha)$

$$\left[ \tilde{\theta}_{0,j} \pm c_{1-\alpha} \cdot \hat{\sigma}_j/\sqrt{N} \right] \text{ for } j=1, \ldots, p_1.$$

Para demonstrar o bootstrap, simulamos dados de um modelo de regressão parcialmente linear esparso. Então estimamos o modelo PLR e realizamos o bootstrap com multiplicadores. Intervalos de confiança conjuntos baseados no bootstrap com multiplicadores são então obtidos definindo a opção `joint` ao chamar o método `confint`.

Além disso, um ajuste de teste de múltiplas hipóteses de valores-p de um modelo de alta dimensão pode ser obtido com o método `p_adjust`. [DoubleML](../index.html#doubleml-package) realiza uma versão do ajuste stepdown Romano-Wolf, que é baseado no bootstrap com multiplicadores, por padrão. Alternativamente, `p_adjust` permite aos usuários aplicar correções tradicionais via a opção `method`.

```{r, message = FALSE, warning = FALSE}

library(DoubleML)

library(mlr3)

library(mlr3learners)

library(data.table)

lgr::get_logger("mlr3")$set_threshold("warn")

set.seed(3141)

n_obs = 500

n_vars = 100

theta = rep(3, 3)

X = matrix(stats::rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)

y = X[, 1:3, drop = FALSE] %*% theta + stats::rnorm(n_obs)

dml_data = double_ml_data_from_matrix(X = X[, 11:n_vars], y = y, d = X[,1:10])

learner = lrn("regr.cv_glmnet", s = "lambda.min")

ml_l = learner$clone()

ml_m = learner$clone()

dml_plr = DoubleMLPLR$new(dml_data, ml_l, ml_m)

dml_plr$fit()

dml_plr$bootstrap()

dml_plr$confint(joint = TRUE)

```

```{r, message = FALSE, warning = FALSE}

dml_plr$p_adjust()

```

```{r, message = FALSE, warning = FALSE}

dml_plr$p_adjust(method = "bonferroni")

```

## 8.3 - Inferência Simultânea em Diferentes Modelos DoubleML (Avançado)

O pacote [DoubleML](../index.html#doubleml-package) fornece um método para realizar inferência simultânea válida sobre diferentes modelos DoubleML.

**Nota:** Note que os intervalos de confiança geralmente serão válidos apenas se as suposições mais fortes (uniformes) sobre, por exemplo, estimativas nuisance forem satisfeitas. Além disso, os modelos devem ser estimados no mesmo conjunto de dados.

A classe `doubleml.DoubleML` contém um atributo `framework` que armazena um objeto `doubleml.DoubleMLFramework`. Este objeto contém uma versão escalonada da função score que é usada para construir intervalos de confiança. Os objetos framework podem ser concatenados usando a função `doubleml.concat()`.

**Exemplo em Python:**

``` python
import doubleml as dml
import numpy as np
from sklearn.base import clone
from sklearn.linear_model import LassoCV
from sklearn.ensemble import RandomForestRegressor

# Simular dados
np.random.seed(1234)
n_obs = 500
n_vars = 100
X = np.random.normal(size=(n_obs, n_vars))
theta = np.array([3., 3., 3.])
y = np.dot(X[:, :3], theta) + np.random.standard_normal(size=(n_obs,))
dml_data = dml.DoubleMLData.from_arrays(X[:, 10:], y, X[:, :10])

learner = LassoCV()
dml_plr_1 = dml.DoubleMLPLR(dml_data, clone(learner), clone(learner))

learner_rf = RandomForestRegressor()
dml_plr_2 = dml.DoubleMLPLR(dml_data, clone(learner_rf), clone(learner_rf))

dml_plr_1.fit()
dml_plr_2.fit()

dml_combined = dml.concat([dml_plr_1.framework, dml_plr_2.framework])
dml_combined.bootstrap().confint(joint=True)
```

Frameworks também podem ser adicionados ou subtraídos uns dos outros. Obviamente, isso muda o parâmetro estimado e deve ser usado com cautela.

``` python
# Continuando o exemplo anterior
dml_combined = dml_plr_1.framework - dml_plr_2.framework
dml_combined.bootstrap().confint(joint=True)
```

Um possível caso de uso é subtrair as estimativas de dois modelos de resultado potencial médio como, por exemplo, no exemplo `DoubleMLQTE`. Isso também funciona para múltiplas repetições se ambos os modelos tiverem o mesmo número de repetições, pois cada repetição é tratada separadamente.

## 8.4 - Referências

Belloni, A., Chernozhukov, V., Chetverikov, D., Wei, Y. (2018), Uniformly valid post-regularization confidence regions for many functional parameters in z-estimation framework. The Annals of Statistics, 46 (6B): 3643-75, [doi: 10.1214/17-AOS1671](https://doi.org/10.1214/17-AOS1671).

Chernozhukov, V., Chetverikov, D., Kato, K. (2013). Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors. The Annals of Statistics 41 (6): 2786-2819, [doi: 10.1214/13-AOS1161](https://dx.doi.org/10.1214/13-AOS1161).

Chernozhukov, V., Chetverikov, D., Kato, K. (2014), Gaussian approximation of suprema of empirical processes. The Annals of Statistics 42 (4): 1564-97, [doi: 10.1214/14-AOS1230](https://dx.doi.org/10.1214/14-AOS1230).
